{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target var\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.style as style\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "def plot_target(df,target):\n",
    "    y= df['target']\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=True,figsize=(15,8))\n",
    "    grid = gridspec.GridSpec(ncols=3,nrows=3,figure=fig)\n",
    "    \n",
    "    # histogram\n",
    "    ax1 = fig.add_subplot(grid[0,:])\n",
    "    ax1.set_title('Histogram')\n",
    "    sns.distplot(y,norm_hist=True,ax=ax1)\n",
    "    \n",
    "    #qq plot\n",
    "    ax2 =fig.add_subplot(grid[1,:2])\n",
    "    ax2.set_title('QQ Plot')\n",
    "    scipy.stats.probplot(y,plot=ax2)\n",
    "    \n",
    "    # box plot\n",
    "    ax3 = fig.add_subplot(grid[:,2])\n",
    "    ax3.set_title('Box Plot')\n",
    "    sns.boxplot(y,orient='v',ax=ax3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot keras history\n",
    "def plot_history(history: keras.callbacks.History):\n",
    "    \"\"\"\n",
    "    Plots Keras hisory\n",
    "    \"\"\"\n",
    "    metrics = [metric for metric in history.history.keys() if not metric.startswith('val_')]\n",
    "    stride = len(history.epoch)//20\n",
    "    plotted_epochs = history.epoch[::stride]\n",
    "    \n",
    "    fig, subplots = plt.subplots(len(metrics), figsize=(8, 4*len(metrics)))\n",
    "    subplots = subplots if len(metrics) != 1 else (subplots,)\n",
    "    fig.tight_layout(h_pad=3, rect=[0, 0, 1, 0.95])\n",
    "    fig.suptitle('Model training history', fontsize=18)\n",
    "    \n",
    "    for metric, subplot in zip(metrics, subplots):\n",
    "        subplot.plot(plotted_epochs, history.history[metric][::stride], marker='.')\n",
    "        try: subplot.plot(plotted_epochs, history.history[f'val_{metric}'], marker='.')\n",
    "        except KeyError: pass\n",
    "        subplot.set_xticks(plotted_epochs)\n",
    "        subplot.set_ylabel(metric)\n",
    "        subplot.set_xlabel('epoch')\n",
    "    \n",
    "    if len(metrics) != len(history.history):\n",
    "        fig.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "    \n",
    "# PLOT CORRELATIONS\n",
    "def plot_chi2_heatmap(df, columns_to_compare):\n",
    "    # columns_to_compare e.g. df.columns.values\n",
    "    \n",
    "\n",
    "    factors_paired = [(i, j) for i in columns_to_compare for j in columns_to_compare]\n",
    "\n",
    "    chi2, p_values = [], []\n",
    "\n",
    "    for f in factors_paired:\n",
    "        if f[0] != f[1]:\n",
    "            chitest = chi2_contingency(pd.crosstab(df[f[0]], df[f[1]]))\n",
    "            chi2.append(chitest[0])\n",
    "            p_values.append(chitest[1])\n",
    "        else:\n",
    "            chi2.append(0)\n",
    "            p_values.append(0)\n",
    "\n",
    "    chi2 = np.array(chi2).reshape((len(columns_to_compare), len(columns_to_compare)))  # shape it as a matrix\n",
    "    chi2 = pd.DataFrame(chi2, index=columns_to_compare, columns=columns_to_compare)\n",
    "    sns.heatmap(chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downcast dataframe to save memory usage\n",
    "def downcast_dtypes(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform TSNE\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne(X, y, perplexity=100, learning_rate=200, n_components=2):\n",
    "    tsne = TSNE(n_components=n_components, init='random',\n",
    "                random_state=None, perplexity=perplexity, verbose=1)\n",
    "    result = tsne.fit_transform(X)\n",
    "    result = pd.DataFrame(result)\n",
    "    result = result.join(y)\n",
    "    result.columns = ['x0', 'x1', 'y']\n",
    "    sns.lmplot('x0', 'x1', result, fit_reg=False, hue='y', palette={0:\"#2662c1\", 1:\"#c9001e\"},\n",
    "              scatter_kws={'alpha': .5})\n",
    "    plt.title('t-SNE plot')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_dummies(df):\n",
    "    \"\"\"\n",
    "    :param df: Pandas DataFrame\n",
    "    :return: Pandas Dataframe()\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(df, columns=list(df.select_dtypes(include='category').columns), drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_remove_novariance(df):\n",
    "    \"\"\"\n",
    "    :param df: Pandas DataFrame\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    uniques = df.apply(pd.Series.nunique)\n",
    "    return df.drop(columns=list(uniques[uniques == 1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get feature importance\n",
    "def get_rf_feat_importances(X,y):\n",
    "    rf = RandomForestClassifier(n_estimators=20, random_state = 42)\n",
    "    rf.fit(X, y)\n",
    "    df = pd.DataFrame(\n",
    "        {'feature': X.columns, 'importance':rf.feature_importances_})\n",
    "    df = df.sort_values(by=['importance'], ascending=False)\n",
    "    return df\n",
    "\n",
    "def plot_feature_importance_gbc(clf, feature_names, topk = 25, figsize = (50,70) ):\n",
    "    #topk = 25\n",
    "    fig = plt.figure(figsize = figsize)\n",
    "    importances = clf.feature_importances_ \n",
    "    sorted_idx = np.argsort(importances)[-topk:]\n",
    "    #sorted_idx = sorted_idx[::-1]\n",
    "    padding = np.arange(len(sorted_idx)) + 0.5\n",
    "    #plt.barh(padding, importances[sorted_idx], align='center')\n",
    "    plt.barh(padding, importances[sorted_idx],\\\n",
    "       color=\"b\", alpha = 0.5, align=\"center\")    \n",
    "    plt.tick_params(axis='y', which='major', labelsize=10)\n",
    "    plt.yticks(padding, feature_names[sorted_idx])\n",
    "    #plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_feature_importance(rf, feature_names, topk = 25, errorbar=False, figsize = (50,70) ):\n",
    "    #topk = 25\n",
    "    fig = plt.figure(figsize = figsize)\n",
    "    importances = rf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)    \n",
    "    sorted_idx = np.argsort(importances)[-topk:]\n",
    "    padding = np.arange(len(sorted_idx)) + 0.5\n",
    "    #plt.barh(padding, importances[sorted_idx], align='center')\n",
    "    if errorbar: \n",
    "        plt.barh(padding, importances[sorted_idx],\\\n",
    "            color=\"b\", alpha = 0.5, xerr=std[sorted_idx], align=\"center\")   \n",
    "    else:\n",
    "        plt.barh(padding, importances[sorted_idx],\\\n",
    "        color=\"b\", alpha = 0.5, align=\"center\")  \n",
    "    plt.tick_params(axis='y', which='major', labelsize=10)\n",
    "    plt.yticks(padding, feature_names[sorted_idx])\n",
    "    plt.show()\n",
    "    #plt.plot()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print False Postive and False Negative samples\n",
    "def get_fp_fn_samples(test_y, test_y_pred, test_txt):\n",
    "\n",
    "    i_lst_fp = [i for i in xrange(len(test_y)) if test_y[i] == 0 and test_y_pred[i] == 1]\n",
    "    i_lst_fn = [i for i in xrange(len(test_y)) if test_y[i] == 1 and test_y_pred[i] == 0]\n",
    "    print '\\nfalse positive'\n",
    "    for i in i_lst_fp[:20]:\n",
    "        print i, test_y[i], ':', test_txt[i]\n",
    "    print 'false negative'\n",
    "    for i in i_lst_fn[:20]:\n",
    "        print i, test_y[i], ':', test_txt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, outlier_column_name, drop_anomalies=False, threshold=3):\n",
    "    \"\"\"\n",
    "    Given a dataframe, remove outliers from a given column, according to some threshold.\n",
    "    Return a dataframe.\n",
    "    \"\"\"\n",
    "    from scipy.stats import zscore\n",
    "    z_name = outlier_column_name + '_z'\n",
    "    df[z_name] = df[[outlier_column_name]].apply(zscore)\n",
    "    initial = df.shape[0]\n",
    "    if drop_anomalies:\n",
    "        df = df[(abs(df[z_name]) < threshold)]\n",
    "        df = df.drop(z_name, axis=1)\n",
    "        after = initial - df.shape[0]\n",
    "        print(f\"{after} outliers for {outlier_column_name} have been removed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_entries_outside_iq_range(df, col):\n",
    "    q1 = df.diff_entry.quantile(0.25)\n",
    "    q3 = df.diff_entry.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    iq_rem = df[~((df[col] < (q1 - iqr)) | (df[col] > (q3 + iqr)))]\n",
    "    return iq_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from metrics_helper import get_confusion_rates\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "def plot_class_hist(data, target, feature, kde=False):\n",
    "    \"\"\"\n",
    "    In a binary classification setting this function plots \n",
    "    two histograms of a given variable grouped by a class label.\n",
    "    \n",
    "    It is a wrapper around Seaborn's .distplot()\n",
    "    \n",
    "    Parameters:\n",
    "    data    : name of your pd.DataFrame\n",
    "    target  : name of a target column in data (string)\n",
    "    feature : name of a feature column you want to plot (string)\n",
    "    kde     : if you want to plot density estimation (boolean)\n",
    "    (C) Aleksander Molak, 2018 MIT License || https://github.com/AlxndrMlk/\n",
    "    \"\"\"\n",
    "    \n",
    "    sns.distplot(data[data[target]==1][feature],\\\n",
    "                 label='1', color='#b71633', norm_hist=True, kde=kde)\n",
    "    sns.distplot(data[data[target]==0][feature],\\\n",
    "                 label='0', color='#417adb', norm_hist=True, kde=kde)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(feature)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc(y, y_pred_prob):\n",
    "    '''\n",
    "    for binary classification\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.4f)' % ( roc_auc))    \n",
    "\n",
    "def plot_roc_cv(classifier, X, y, cv):\n",
    "    '''\n",
    "    cv = KFold(len(y),n_folds=5)\n",
    "    '''\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "    mean_tpr /= len(cv)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
